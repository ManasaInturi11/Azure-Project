
 


ğŸš€ Azure ETL Pipeline: Data Extraction, Transformation & Load

ğŸ“Œ Project Overview
This project builds an ETL pipeline using Azure Data Factory (ADF), Databricks, and PySpark** to extract data from SQL Server & GitHub, transform it via Medallion Architecture (Bronze â†’ Silver â†’ Gold), and handle incremental loads using SCD Type 1 (Upsert).  

---

 ğŸ—ï¸ Architecture & Workflow

 ğŸ”¹ Data Sources
 
- SQL Server â€“ Relational database  
- GitHub â€“ Raw JSON data extraction  

 ğŸ”¹ Pipeline Workflow
 
1ï¸âƒ£ Data Ingestion â€“ Extracting data using **Azure Data Factory (ADF)

2ï¸âƒ£ Staging â€“ Copying raw data into a staging table

3ï¸âƒ£ Incremental Load Management â€“ Handling changes using SCD Type 1 (Upsert) 

4ï¸âƒ£ Data Transformation â€“ Processing data in Databricks using PySpark & Delta Lake 

5ï¸âƒ£ Final Storage â€“ Organizing data into **fact & dimension tables** for analytics  

 ğŸ”¹Medallion Architecture Implementation 
- Bronze Layer â†’ Raw ingested data  
- Silver Layer â†’ Cleaned & structured data  
- Gold Layer â†’ Aggregated & analytics-ready data  

---

ğŸ› ï¸ Technologies Used  
- Azure Data Factory (ADF) â€“ Data orchestration & pipeline execution  
- Azure Databricks â€“ Data transformation with PySpark  
- Delta Lake & Delta Tables â€“ Optimized storage & performance  
- SQL Server â€“ Structured data storage  

---

 ğŸ“Œ Key Features:
 
âœ… Incremental data loads using a watermark column 

âœ… SCD Type 1 (Upsert) for efficient data updates

âœ… Optimized pipeline execution with parallel processing

âœ… Error handling & schema evolution in Delta Tables 

---

 ğŸ† Challenges & Solutions  

ğŸ”¹Handling Incremental Loads

âœ” Implemented a watermark table to track `last_load_timestamp`  

âœ” Ensured SCD Type 1 Upsert** for efficient record updates  

ğŸ”¹ Parallel Execution Issues

âœ” Resolved data conflicts via deduplication strategies in `fact_sales` 

âœ” Maintained **consistent schema evolution** in Delta Tables  

